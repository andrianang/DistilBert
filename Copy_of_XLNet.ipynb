{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "cred = {\"username\":\"hanunshaka\",\"key\":\"763365a03eae6d3aabd49c0a6a2559a5\"}"
      ],
      "metadata": {
        "id": "cq2VJmFaZ2jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle/\n",
        "!touch ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "BJjtm2KDZ50G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_token = cred\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "sTZhiZz8Z80l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !kaggle datasets download -d jp797498e/twitter-entity-sentiment-analysis\n",
        "\n",
        " !unzip twitter-entity-sentiment-analysis.zip"
      ],
      "metadata": {
        "id": "bNyux_i7Z_q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8QfjykKlPqG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd             # Untuk manipulasi dan analisis data I/O dataframe (ex read dataset)\n",
        "import numpy as np              # Untuk operasi numerik dan komputasi\n",
        "import matplotlib.pyplot as plt # Untuk visualisasi grafik dan plot\n",
        "import seaborn as sns           # Dibangun di atas plt, menyediakan visualisasi yang lebih menarik\n",
        "import warnings                 # Handling peringatan yang ditampilkan\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('twitter_training.csv', error_bad_lines=False)\n",
        "train_df.columns = ['TweetID', 'Entity', 'Sentiment', 'Content']\n",
        "train_df"
      ],
      "metadata": {
        "id": "ZSRpXPnu0vlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "VY9sAdK90v5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('twitter_validation.csv')\n",
        "test_df.columns = ['TweetID', 'Entity', 'Sentiment', 'Content']\n",
        "test_df"
      ],
      "metadata": {
        "id": "WIs_eN550xrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.info()"
      ],
      "metadata": {
        "id": "Ge8g7E_h0zA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_row = train_df[train_df['Content'].isna()]\n",
        "nan_row"
      ],
      "metadata": {
        "id": "hUEymeM204-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop(nan_row.index, axis=0, inplace=True)\n",
        "train_df.info()"
      ],
      "metadata": {
        "id": "LnXAOu0kgaMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_duplicate = train_df[train_df.duplicated(subset=['Entity', 'Sentiment', 'Content'], keep=False)]\n",
        "train_duplicate"
      ],
      "metadata": {
        "id": "5O87FwWvgduz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_duplicate = test_df[test_df.duplicated(subset=['Entity', 'Sentiment', 'Content'], keep=False)]\n",
        "test_duplicate"
      ],
      "metadata": {
        "id": "FXNVKGFAgjvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop(train_duplicate.index, axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "ObFwcVCIgocX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "_YtZ3m5v1Rgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.info()"
      ],
      "metadata": {
        "id": "e-CdKJWY3hiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = train_df['Sentiment'].value_counts()\n",
        "\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
        "\n",
        "plt.title('Sentiment Distribution Train')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C2lNL0n7ss5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Selection"
      ],
      "metadata": {
        "id": "N3cFyXmW3lq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop('TweetID', axis=1, inplace=True)\n",
        "# train_df.info()"
      ],
      "metadata": {
        "id": "lviXAtBJ1Vfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Emoji"
      ],
      "metadata": {
        "id": "hv2ibhwFs6KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install emot"
      ],
      "metadata": {
        "id": "zMn7KM8o3KUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "from tqdm import tqdm\n",
        "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "43XyGM7qsq2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "cwDKLHyFtYHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "def convert_emojis(text):\n",
        "    text_list = word_tokenize(text)\n",
        "    new_list = [UNICODE_EMOJI[word.lower()] if word.lower() in UNICODE_EMOJI.keys() else word for word in text_list]\n",
        "    return  \" \".join(new_list)\n",
        "\n",
        "def convert_emoticons(text):\n",
        "    text_list = word_tokenize(text)\n",
        "    new_list = [EMOTICONS_EMO[word.lower()] if word.lower() in EMOTICONS_EMO.keys() else word for word in text_list]\n",
        "    return  \" \".join(new_list)\n",
        "\n",
        "\n",
        "# Apply transformations with progress bars\n",
        "train_df['emo_content'] = train_df['Content'].progress_apply(convert_emojis)\n",
        "train_df['emo_cotent'] = train_df['Content'].progress_apply(convert_emoticons)\n",
        "\n",
        "test_df['emo_content'] = test_df['Content'].progress_apply(convert_emojis)\n",
        "test_df['emo_cotent'] = test_df['Content'].progress_apply(convert_emoticons)\n",
        "\n",
        "train_df['emo_content']\n"
      ],
      "metadata": {
        "id": "UxL0QDZM32A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abbreviations = {\n",
        "    \"$\" : \" dollar \",\n",
        "    \"â‚¬\" : \" euro \",\n",
        "    \"4ao\" : \"for adults only\",\n",
        "    \"a.m\" : \"before midday\",\n",
        "    \"a3\" : \"anytime anywhere anyplace\",\n",
        "    \"aamof\" : \"as a matter of fact\",\n",
        "    \"acct\" : \"account\",\n",
        "    \"adih\" : \"another day in hell\",\n",
        "    \"afaic\" : \"as far as i am concerned\",\n",
        "    \"afaict\" : \"as far as i can tell\",\n",
        "    \"afaik\" : \"as far as i know\",\n",
        "    \"afair\" : \"as far as i remember\",\n",
        "    \"afk\" : \"away from keyboard\",\n",
        "    \"app\" : \"application\",\n",
        "    \"approx\" : \"approximately\",\n",
        "    \"apps\" : \"applications\",\n",
        "    \"asap\" : \"as soon as possible\",\n",
        "    \"asl\" : \"age, sex, location\",\n",
        "    \"atk\" : \"at the keyboard\",\n",
        "    \"ave.\" : \"avenue\",\n",
        "    \"aymm\" : \"are you my mother\",\n",
        "    \"ayor\" : \"at your own risk\",\n",
        "    \"b&b\" : \"bed and breakfast\",\n",
        "    \"b+b\" : \"bed and breakfast\",\n",
        "    \"b.c\" : \"before christ\",\n",
        "    \"b2b\" : \"business to business\",\n",
        "    \"b2c\" : \"business to customer\",\n",
        "    \"b4\" : \"before\",\n",
        "    \"b4n\" : \"bye for now\",\n",
        "    \"b@u\" : \"back at you\",\n",
        "    \"bae\" : \"before anyone else\",\n",
        "    \"bak\" : \"back at keyboard\",\n",
        "    \"bbbg\" : \"bye bye be good\",\n",
        "    \"bbc\" : \"british broadcasting corporation\",\n",
        "    \"bbias\" : \"be back in a second\",\n",
        "    \"bbl\" : \"be back later\",\n",
        "    \"bbs\" : \"be back soon\",\n",
        "    \"be4\" : \"before\",\n",
        "    \"bfn\" : \"bye for now\",\n",
        "    \"blvd\" : \"boulevard\",\n",
        "    \"bout\" : \"about\",\n",
        "    \"brb\" : \"be right back\",\n",
        "    \"bros\" : \"brothers\",\n",
        "    \"brt\" : \"be right there\",\n",
        "    \"bsaaw\" : \"big smile and a wink\",\n",
        "    \"btw\" : \"by the way\",\n",
        "    \"bwl\" : \"bursting with laughter\",\n",
        "    \"c/o\" : \"care of\",\n",
        "    \"cet\" : \"central european time\",\n",
        "    \"cf\" : \"compare\",\n",
        "    \"cia\" : \"central intelligence agency\",\n",
        "    \"csl\" : \"can not stop laughing\",\n",
        "    \"cu\" : \"see you\",\n",
        "    \"cul8r\" : \"see you later\",\n",
        "    \"cv\" : \"curriculum vitae\",\n",
        "    \"cwot\" : \"complete waste of time\",\n",
        "    \"cya\" : \"see you\",\n",
        "    \"cyt\" : \"see you tomorrow\",\n",
        "    \"dae\" : \"does anyone else\",\n",
        "    \"dbmib\" : \"do not bother me i am busy\",\n",
        "    \"diy\" : \"do it yourself\",\n",
        "    \"dm\" : \"direct message\",\n",
        "    \"dwh\" : \"during work hours\",\n",
        "    \"e123\" : \"easy as one two three\",\n",
        "    \"eet\" : \"eastern european time\",\n",
        "    \"eg\" : \"example\",\n",
        "    \"embm\" : \"early morning business meeting\",\n",
        "    \"encl\" : \"enclosed\",\n",
        "    \"encl.\" : \"enclosed\",\n",
        "    \"etc\" : \"and so on\",\n",
        "    \"faq\" : \"frequently asked questions\",\n",
        "    \"fawc\" : \"for anyone who cares\",\n",
        "    \"fb\" : \"facebook\",\n",
        "    \"fc\" : \"fingers crossed\",\n",
        "    \"fig\" : \"figure\",\n",
        "    \"fimh\" : \"forever in my heart\",\n",
        "    \"ft.\" : \"feet\",\n",
        "    \"ft\" : \"featuring\",\n",
        "    \"ftl\" : \"for the loss\",\n",
        "    \"ftw\" : \"for the win\",\n",
        "    \"fwiw\" : \"for what it is worth\",\n",
        "    \"fyi\" : \"for your information\",\n",
        "    \"g9\" : \"genius\",\n",
        "    \"gahoy\" : \"get a hold of yourself\",\n",
        "    \"gal\" : \"get a life\",\n",
        "    \"gcse\" : \"general certificate of secondary education\",\n",
        "    \"gfn\" : \"gone for now\",\n",
        "    \"gg\" : \"good game\",\n",
        "    \"gl\" : \"good luck\",\n",
        "    \"glhf\" : \"good luck have fun\",\n",
        "    \"gmt\" : \"greenwich mean time\",\n",
        "    \"gmta\" : \"great minds think alike\",\n",
        "    \"gn\" : \"good night\",\n",
        "    \"g.o.a.t\" : \"greatest of all time\",\n",
        "    \"goat\" : \"greatest of all time\",\n",
        "    \"goi\" : \"get over it\",\n",
        "    \"gps\" : \"global positioning system\",\n",
        "    \"gr8\" : \"great\",\n",
        "    \"gratz\" : \"congratulations\",\n",
        "    \"gyal\" : \"girl\",\n",
        "    \"h&c\" : \"hot and cold\",\n",
        "    \"hp\" : \"horsepower\",\n",
        "    \"hr\" : \"hour\",\n",
        "    \"hrh\" : \"his royal highness\",\n",
        "    \"ht\" : \"height\",\n",
        "    \"ibrb\" : \"i will be right back\",\n",
        "    \"ic\" : \"i see\",\n",
        "    \"icq\" : \"i seek you\",\n",
        "    \"icymi\" : \"in case you missed it\",\n",
        "    \"idc\" : \"i do not care\",\n",
        "    \"idgadf\" : \"i do not give a damn fuck\",\n",
        "    \"idgaf\" : \"i do not give a fuck\",\n",
        "    \"idk\" : \"i do not know\",\n",
        "    \"ie\" : \"that is\",\n",
        "    \"i.e\" : \"that is\",\n",
        "    \"ifyp\" : \"i feel your pain\",\n",
        "    \"IG\" : \"instagram\",\n",
        "    \"iirc\" : \"if i remember correctly\",\n",
        "    \"ilu\" : \"i love you\",\n",
        "    \"ily\" : \"i love you\",\n",
        "    \"imho\" : \"in my humble opinion\",\n",
        "    \"imo\" : \"in my opinion\",\n",
        "    \"imu\" : \"i miss you\",\n",
        "    \"iow\" : \"in other words\",\n",
        "    \"irl\" : \"in real life\",\n",
        "    \"j4f\" : \"just for fun\",\n",
        "    \"jic\" : \"just in case\",\n",
        "    \"jk\" : \"just kidding\",\n",
        "    \"jsyk\" : \"just so you know\",\n",
        "    \"l8r\" : \"later\",\n",
        "    \"lb\" : \"pound\",\n",
        "    \"lbs\" : \"pounds\",\n",
        "    \"ldr\" : \"long distance relationship\",\n",
        "    \"lmao\" : \"laugh my ass off\",\n",
        "    \"lmfao\" : \"laugh my fucking ass off\",\n",
        "    \"lol\" : \"laughing out loud\",\n",
        "    \"ltd\" : \"limited\",\n",
        "    \"ltns\" : \"long time no see\",\n",
        "    \"m8\" : \"mate\",\n",
        "    \"mf\" : \"motherfucker\",\n",
        "    \"mfs\" : \"motherfuckers\",\n",
        "    \"mfw\" : \"my face when\",\n",
        "    \"mofo\" : \"motherfucker\",\n",
        "    \"mph\" : \"miles per hour\",\n",
        "    \"mr\" : \"mister\",\n",
        "    \"mrw\" : \"my reaction when\",\n",
        "    \"ms\" : \"miss\",\n",
        "    \"mte\" : \"my thoughts exactly\",\n",
        "    \"nagi\" : \"not a good idea\",\n",
        "    \"nbc\" : \"national broadcasting company\",\n",
        "    \"nbd\" : \"not big deal\",\n",
        "    \"nfs\" : \"not for sale\",\n",
        "    \"ngl\" : \"not going to lie\",\n",
        "    \"nhs\" : \"national health service\",\n",
        "    \"nrn\" : \"no reply necessary\",\n",
        "    \"nsfl\" : \"not safe for life\",\n",
        "    \"nsfw\" : \"not safe for work\",\n",
        "    \"nth\" : \"nice to have\",\n",
        "    \"nvr\" : \"never\",\n",
        "    \"nyc\" : \"new york city\",\n",
        "    \"oc\" : \"original content\",\n",
        "    \"og\" : \"original\",\n",
        "    \"ohp\" : \"overhead projector\",\n",
        "    \"oic\" : \"oh i see\",\n",
        "    \"omdb\" : \"over my dead body\",\n",
        "    \"omg\" : \"oh my god\",\n",
        "    \"omw\" : \"on my way\",\n",
        "    \"p.a\" : \"per annum\",\n",
        "    \"p.m\" : \"after midday\",\n",
        "    \"pm\" : \"prime minister\",\n",
        "    \"poc\" : \"people of color\",\n",
        "    \"pov\" : \"point of view\",\n",
        "    \"pp\" : \"pages\",\n",
        "    \"ppl\" : \"people\",\n",
        "    \"prw\" : \"parents are watching\",\n",
        "    \"ps\" : \"postscript\",\n",
        "    \"pt\" : \"point\",\n",
        "    \"ptb\" : \"please text back\",\n",
        "    \"pto\" : \"please turn over\",\n",
        "    \"qpsa\" : \"what happens\",\n",
        "    \"ratchet\" : \"rude\",\n",
        "    \"rbtl\" : \"read between the lines\",\n",
        "    \"rlrt\" : \"real life retweet\",\n",
        "    \"rofl\" : \"rolling on the floor laughing\",\n",
        "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\" : \"retweet\",\n",
        "    \"ruok\" : \"are you ok\",\n",
        "    \"sfw\" : \"safe for work\",\n",
        "    \"sk8\" : \"skate\",\n",
        "    \"smh\" : \"shake my head\",\n",
        "    \"sq\" : \"square\",\n",
        "    \"srsly\" : \"seriously\",\n",
        "    \"ssdd\" : \"same stuff different day\",\n",
        "    \"tbh\" : \"to be honest\",\n",
        "    \"tbs\" : \"tablespooful\",\n",
        "    \"tbsp\" : \"tablespooful\",\n",
        "    \"tfw\" : \"that feeling when\",\n",
        "    \"thks\" : \"thank you\",\n",
        "    \"tho\" : \"though\",\n",
        "    \"thx\" : \"thank you\",\n",
        "    \"tia\" : \"thanks in advance\",\n",
        "    \"til\" : \"today i learned\",\n",
        "    \"tl;dr\" : \"too long i did not read\",\n",
        "    \"tldr\" : \"too long i did not read\",\n",
        "    \"tmb\" : \"tweet me back\",\n",
        "    \"tntl\" : \"trying not to laugh\",\n",
        "    \"ttyl\" : \"talk to you later\",\n",
        "    \"u\" : \"you\",\n",
        "    \"u2\" : \"you too\",\n",
        "    \"u4e\" : \"yours for ever\",\n",
        "    \"utc\" : \"coordinated universal time\",\n",
        "    \"w/\" : \"with\",\n",
        "    \"w/o\" : \"without\",\n",
        "    \"w8\" : \"wait\",\n",
        "    \"wassup\" : \"what is up\",\n",
        "    \"wb\" : \"welcome back\",\n",
        "    \"wtf\" : \"what the fuck\",\n",
        "    \"wtg\" : \"way to go\",\n",
        "    \"wtpa\" : \"where the party at\",\n",
        "    \"wuf\" : \"where are you from\",\n",
        "    \"wuzup\" : \"what is up\",\n",
        "    \"wywh\" : \"wish you were here\",\n",
        "    \"yd\" : \"yard\",\n",
        "    \"ygtr\" : \"you got that right\",\n",
        "    \"ynk\" : \"you never know\",\n",
        "    \"zzz\" : \"sleeping bored and tired\"\n",
        "}"
      ],
      "metadata": {
        "id": "YNbmV4nIteB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_abbrev(word):\n",
        "    word_list = word_tokenize(word)\n",
        "    new_list = [abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word for word in word_list]\n",
        "    return \" \".join(new_list)\n",
        "\n",
        "train_df['abbrev_content'] = train_df['emo_content'].apply(convert_abbrev)\n",
        "test_df['abbrev_content'] = test_df['emo_content'].apply(convert_abbrev)"
      ],
      "metadata": {
        "id": "wjCmukja4Q2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "def remove_extra(input_string):\n",
        "    #Remove link\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text_without_link = re.sub(url_pattern, '', input_string)\n",
        "\n",
        "    # Remove \"@\" symbols\n",
        "    text_without_ad = text_without_link.replace('@', '')\n",
        "\n",
        "    #Remove Hastag\n",
        "    hashtag_pattern = re.compile(r'#\\w+')\n",
        "    text_without_hastag = re.sub(hashtag_pattern, '', text_without_ad)\n",
        "\n",
        "    punctuation_except_comma_period = '[%s]' % re.escape(''.join(c for c in string.punctuation))\n",
        "    text_without_special_char = re.sub(punctuation_except_comma_period, '', text_without_hastag)\n",
        "\n",
        "    text_without_extra_space = re.sub(' +', ' ', text_without_special_char)\n",
        "    return text_without_extra_space\n",
        "\n",
        "train_df['delete_extra'] = train_df['abbrev_content'].apply(remove_extra)\n",
        "test_df['delete_extra'] = test_df['abbrev_content'].apply(remove_extra)\n",
        "train_df['delete_extra']"
      ],
      "metadata": {
        "id": "xiOuP6pH47eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "4u6B2X-U4197"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "def expand_shortcut(text) :\n",
        "    expanded_words = []\n",
        "    for word in text.split():\n",
        "        expanded_words.append(contractions.fix(word))\n",
        "\n",
        "    return ' '.join(expanded_words)\n",
        "\n",
        "train_df['expanded_content'] = train_df['delete_extra'].apply(expand_shortcut)\n",
        "test_df['expanded_content'] = test_df['delete_extra'].apply(expand_shortcut)\n",
        "\n",
        "train_df['expanded_content']"
      ],
      "metadata": {
        "id": "fqfsr_upttkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['lower_content'] = train_df['expanded_content'].apply(lambda x: x.lower())\n",
        "test_df['lower_content'] = test_df['expanded_content'].apply(lambda x: x.lower())\n",
        "\n",
        "train_df['lower_content']"
      ],
      "metadata": {
        "id": "gpKfDLlXtwXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "sKIak-optyqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "train_df['tokenized_content'] = train_df['lower_content'].apply(word_tokenize)\n",
        "test_df['tokenized_content'] = test_df['lower_content'].apply(word_tokenize)\n",
        "\n",
        "train_df['tokenized_content']"
      ],
      "metadata": {
        "id": "mQyVdRmEt08L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    nltk.data.find('wordnet.zip')\n",
        "except:\n",
        "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
        "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
        "    subprocess.run(command.split())\n",
        "    nltk.data.path.append('/kaggle/working/')\n",
        "\n",
        "# Now you can import the NLTK resources as usual\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "5vtQHDrrt2_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return [lemmatizer.lemmatize(word) for word in text]\n",
        "train_df['lemma_content'] = train_df['tokenized_content'].apply(lemmatize_words)\n",
        "test_df['lemma_content'] = test_df['tokenized_content'].apply(lemmatize_words)\n",
        "\n",
        "train_df['lemma_content']"
      ],
      "metadata": {
        "id": "zjMOoGWVt40G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['lemma_content'][11]"
      ],
      "metadata": {
        "id": "5joT8dnzt7Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt search enchant\n",
        "!apt install -y enchant-2\n",
        "!apt install -qq enchant -y\n",
        "!pip install pyenchant"
      ],
      "metadata": {
        "id": "gQMHIMPRt_v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import enchant\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "def is_valid_word(word):\n",
        "    dictionary = enchant.Dict(\"en_US\")\n",
        "    return dictionary.check(word)\n",
        "\n",
        "def check_en_word(li):\n",
        "    new_list = [w for w in li if is_valid_word(w)]\n",
        "    return new_list\n",
        "\n",
        "train_df['en_content'] = train_df['lemma_content'].progress_apply(check_en_word)\n",
        "test_df['en_content'] = test_df['lemma_content'].progress_apply(check_en_word)\n"
      ],
      "metadata": {
        "id": "08NXGQZFuBvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['en_content']\n",
        "# train_df"
      ],
      "metadata": {
        "id": "lwLPbL7LuKYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "eCoEu3s-uwRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    filtered_sentence = [w for w in text if not w.lower() in stop_words]\n",
        "    return filtered_sentence\n",
        "\n",
        "train_df['remove_stopwords'] = train_df['en_content'].apply(remove_stopwords)\n",
        "test_df['remove_stopwords'] = test_df['en_content'].apply(remove_stopwords)\n",
        "\n",
        "train_df['remove_stopwords']"
      ],
      "metadata": {
        "id": "AQFjOyHuuyKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df[['Sentiment', 'remove_stopwords']]\n",
        "train_df"
      ],
      "metadata": {
        "id": "ZMmrLyX2u0oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicated = train_df[train_df.duplicated(subset=['remove_stopwords'], keep=False)]\n",
        "duplicated"
      ],
      "metadata": {
        "id": "PADvwGE0u4nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop(duplicated.index, axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "ZHR6FdAou60h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "PX7tN5JUDZQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "def combine_list(entity, lst):\n",
        "    str_list = ' '.join(lst)\n",
        "\n",
        "    return f'{entity} [SEP] {str_list}'\n",
        "\n",
        "train_df.loc[:,'final_result'] = train_df.apply(lambda x: combine_list(x['Entity'], x['en_content']), axis=1)\n",
        "test_df.loc[:,'final_result'] = test_df.apply(lambda x: combine_list(x['Entity'], x['en_content']), axis=1)\n",
        "test_df['final_result']\n",
        "train_df['final_result']"
      ],
      "metadata": {
        "id": "hoR4WL5bu9XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explanatory Data Analysis"
      ],
      "metadata": {
        "id": "FyzFAdj75Y8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = train_df['Sentiment'].value_counts()\n",
        "\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
        "\n",
        "plt.title('Sentiment Distribution Train')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7V5UY4q25Idh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mPCdQ1NH8Ark"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Count\n",
        "train_df['Word_Count'] = train_df.apply(lambda x: len(str(x['Content']).split()), axis=1)\n",
        "train_df['Word_Count'].mean()"
      ],
      "metadata": {
        "id": "xiMQ8Hpq5kfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(21,9))\n",
        "\n",
        "n = 1\n",
        "\n",
        "for i in range(len(train_df['Sentiment'].unique())):\n",
        "  plt.subplot(1, len(train_df['Sentiment'].unique()), n)\n",
        "  n += 1\n",
        "  sns.histplot(x = \"Word_Count\", data=train_df[train_df['Sentiment'] == train_df['Sentiment'].unique()[i]])\n",
        "  plt.title(f'Word Count {train_df[\"Sentiment\"].unique()[i]}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uqHuZ58S77-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analysis Number Of Stopwords"
      ],
      "metadata": {
        "id": "HFAX7wLWAnDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #NEGATIVE\n",
        "\n",
        "# neg_train_df = train_df[train_df[\"Sentiment\"] == \"Negative\"][\"Content\"]\n",
        "\n",
        "# neg_train_words = ' '.join(neg_train_df)\n",
        "# neg_train_words = re.sub(r'[^\\w\\s]', '', neg_train_words.lower())\n",
        "\n",
        "\n",
        "# neg_train_words = neg_train_words.split()\n",
        "\n",
        "# word_freq = Counter(word for word in neg_train_words if word not in STOPWORDS)\n",
        "# word_freq.most_common(10)"
      ],
      "metadata": {
        "id": "3Hj60ZL5BMs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #NEUTRAL\n",
        "\n",
        "# neu_train_df = train_df[train_df[\"Sentiment\"] == \"Neutral\"][\"Content\"]\n",
        "\n",
        "# neu_train_words = ' '.join(neu_train_df)\n",
        "# neu_train_words = re.sub(r'[^\\w\\s]', '', neu_train_words.lower())\n",
        "\n",
        "# neu_train_words = neu_train_words.split()\n",
        "\n",
        "# word_freq = Counter(word for word in neu_train_words if word not in STOPWORDS)\n",
        "# word_freq.most_common(10)"
      ],
      "metadata": {
        "id": "jIRJurBPDiJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #IRRELEVANT\n",
        "\n",
        "# irr_train_df = train_df[train_df[\"Sentiment\"] == \"Irrelevant\"][\"Content\"]\n",
        "\n",
        "# irr_train_words = ' '.join(irr_train_df)\n",
        "# irr_train_words = re.sub(r'[^\\w\\s]', '', irr_train_words.lower())\n",
        "\n",
        "# irr_train_words = irr_train_words.split()\n",
        "\n",
        "# word_freq = Counter(word for word in irr_train_words if word not in STOPWORDS)\n",
        "# word_freq.most_common(10)"
      ],
      "metadata": {
        "id": "3_Ui700dDM5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modeling\n"
      ],
      "metadata": {
        "id": "jbKU0dIqQHjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XLNet"
      ],
      "metadata": {
        "id": "eRajB33zQQqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "XuuBRgLvQMZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from transformers import TFXLNetModel, XLNetTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "RJdqdw0-DWGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "train_df = train_df[[\"Entity\", \"final_result\", \"Sentiment\"]]\n",
        "\n",
        "train_df.columns = [\"Entity\", \"Content\", \"Sentiment\"]\n",
        "\n",
        "data = train_df[[\"Content\", \"Sentiment\"]]\n",
        "\n",
        "data[\"Content\"][0]"
      ],
      "metadata": {
        "id": "OqVnIRC4CtFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "model = TFXLNetModel.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "# Load and preprocess the data\n",
        "# train_df = train_df[[\"Entity\", \"en_content\", \"Sentiment\"]]\n",
        "\n",
        "# train_df.columns = [\"Entity\", \"Content\", \"Sentiment\"]\n",
        "\n",
        "# data = train_df\n",
        "\n",
        "data['Sentiment_label'] = pd.Categorical(data['Sentiment'])\n",
        "data['Sentiment'] = data['Sentiment_label'].cat.codes\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "data_train, data_val = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Extract the training and testing texts and labels\n",
        "train_texts = data_train['Content'].tolist()\n",
        "train_labels = data_train['Sentiment'].tolist()\n",
        "val_texts = data_val['Content'].tolist()\n",
        "val_labels = data_val['Sentiment'].tolist()\n",
        "\n",
        "# Tokenize the texts\n",
        "# Modified code starts here\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=False, max_length=23)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=False, max_length=23)\n",
        "\n",
        "train_input_ids = pad_sequences(train_encodings['input_ids'], maxlen=23, padding='post')\n",
        "train_attention_mask = pad_sequences(train_encodings['attention_mask'], maxlen=23, padding='post')\n",
        "\n",
        "val_input_ids = pad_sequences(val_encodings['input_ids'], maxlen=23, padding='post')\n",
        "val_attention_mask = pad_sequences(val_encodings['attention_mask'], maxlen=23, padding='post')\n",
        "# Modified code ends here\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "num_labels = len(data['Sentiment_label'].cat.categories)\n",
        "train_labels_encoded = tf.one_hot(train_labels, num_labels)\n",
        "val_labels_encoded = tf.one_hot(val_labels, num_labels)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_input_ids, 'attention_mask': train_attention_mask}, train_labels_encoded))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_input_ids, 'attention_mask': val_attention_mask}, val_labels_encoded))\n",
        "\n",
        "# Define the model architecture\n",
        "input_ids = tf.keras.layers.Input(shape=(23,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(23,), dtype=tf.int32, name='attention_mask')\n",
        "output = model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "output = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :])  # Pooling the output\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compile and train the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Use smaller batch size\n",
        "batch_size = 16\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "model.fit(train_dataset.batch(batch_size), epochs=2)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(val_dataset.batch(batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd2nJgIIQGY-",
        "outputId": "1ed11bd6-1d79-4cb3-dda6-3dca54682036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetModel: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFXLNetModel were initialized from the model checkpoint at xlnet-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_2/transformer/mask_emb:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_2/transformer/mask_emb:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_2/transformer/mask_emb:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_2/transformer/mask_emb:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 829/2594 [========>.....................] - ETA: 3:08:31 - loss: 1.3513 - accuracy: 0.4125"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "# Load and preprocess the data\n",
        "data_val = test_df[['Content', 'Sentiment']]\n",
        "data_val['Sentiment_label'] = pd.Categorical(data_val['Sentiment'])\n",
        "data_val['Sentiment'] = data_val['Sentiment_label'].cat.codes\n",
        "\n",
        "test_texts = data_val['Content'].tolist()\n",
        "test_labels = data_val['Sentiment'].tolist()\n",
        "\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=24)\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "num_labels = len(data['Sentiment_label'].cat.categories)\n",
        "val_labels_encoded = tf.one_hot(val_labels, num_labels)\n",
        "test_labels_encoded = tf.one_hot(test_labels, num_labels)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels_encoded))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n",
        "\n",
        "val_predictions = model.predict(val_dataset.batch(64))\n",
        "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
        "test_predictions = model.predict(test_dataset.batch(64))\n",
        "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Convert the predicted labels to their original sentiment categories\n",
        "val_predicted_sentiments = data['Sentiment_label'].cat.categories[val_predicted_labels]\n",
        "test_predicted_sentiments = data['Sentiment_label'].cat.categories[test_predicted_labels]\n",
        "\n",
        "# Convert the true labels to their original sentiment categories\n",
        "val_true_labels = data_val['Sentiment_label']\n",
        "test_true_labels = data_val['Sentiment_label']\n",
        "\n",
        "# Calculate the classification report for the valing set\n",
        "val_classification_rep = classification_report(val_true_labels, val_predicted_sentiments)\n",
        "print(\"Validation Set - Classification Report:\\n\", val_classification_rep)\n",
        "\n",
        "# Generate the confusion matrix for the valdation set\n",
        "val_confusion_mat = confusion_matrix(val_true_labels, val_predicted_sentiments)\n",
        "\n",
        "# Get the unique labels/categories from the true labels\n",
        "labels = np.unique(val_true_labels)\n",
        "\n",
        "# Plot the confusion matrix for the valing set\n",
        "val_display = ConfusionMatrixDisplay(confusion_matrix=val_confusion_mat, display_labels=labels)\n",
        "val_display.plot(cmap='Blues')\n",
        "plt.title(\"valing Set - Confusion Matrix\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# Calculate the classification report for the test set\n",
        "test_classification_rep = classification_report(test_true_labels, test_predicted_sentiments)\n",
        "print(\"Test Set - Classification Report:\\n\", test_classification_rep)\n",
        "\n",
        "# Generate the confusion matrix for the test set\n",
        "test_confusion_mat = confusion_matrix(test_true_labels, test_predicted_sentiments)\n",
        "\n",
        "# Plot the confusion matrix for the test set\n",
        "test_display = ConfusionMatrixDisplay(confusion_matrix=test_confusion_mat, display_labels=labels)\n",
        "test_display.plot(cmap='Blues')\n",
        "plt.title(\"Test Set - Confusion Matrix\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vHmaiKhC8i-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "data_val = test_df[['Content', 'Sentiment']]\n",
        "data_val['Sentiment_label'] = pd.Categorical(data_val['Sentiment'])\n",
        "data_val['Sentiment'] = data_val['Sentiment_label'].cat.codes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Extract the training and testing texts and labels\n",
        "val_texts = data_val['Content'].tolist()\n",
        "val_labels = data_val['Sentiment'].tolist()\n",
        "\n",
        "# Tokenize the texts\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "num_labels = len(data_val['Sentiment_label'].cat.categories)\n",
        "val_labels_encoded = tf.one_hot(val_labels, num_labels)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels_encoded))\n",
        "\n",
        "# Make predictions on the training and test datasets\n",
        "\n",
        "val_predictions = model.predict(val_dataset.batch(64))\n",
        "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
        "\n",
        "# Convert the predicted labels to their original sentiment categories\n",
        "val_predicted_sentiments = data_val['Sentiment_label'].cat.categories[val_predicted_labels]\n",
        "\n",
        "# Convert the true labels to their original sentiment categories\n",
        "val_true_labels = data_val['Sentiment_label']\n",
        "\n",
        "# Calculate the classification report for the training set\n",
        "val_classification_rep = classification_report(val_true_labels, val_predicted_sentiments)\n",
        "print(\"Training Set - Classification Report:\\n\", val_classification_rep)\n",
        "\n",
        "# Generate the confusion matrix for the training set\n",
        "val_confusion_mat = confusion_matrix(val_true_labels, val_predicted_sentiments)\n",
        "\n",
        "# Get the unique labels/categories from the true labels\n",
        "labels = np.unique(val_true_labels)\n",
        "\n",
        "# Plot the confusion matrix for the training set\n",
        "val_display = ConfusionMatrixDisplay(confusion_matrix=val_confusion_mat, display_labels=labels)\n",
        "val_display.plot(cmap='Blues')\n",
        "plt.title(\"Validation Set - Confusion Matrix\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9rA9K_8z8nRL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}