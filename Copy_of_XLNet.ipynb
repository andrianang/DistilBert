{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq2VJmFaZ2jM"
      },
      "outputs": [],
      "source": [
        "cred = {\"username\":\"hanunshaka\",\"key\":\"763365a03eae6d3aabd49c0a6a2559a5\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJjtm2KDZ50G"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle/\n",
        "!touch ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTZhiZz8Z80l"
      },
      "outputs": [],
      "source": [
        "api_token = cred\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNyux_i7Z_q4"
      },
      "outputs": [],
      "source": [
        " !kaggle datasets download -d jp797498e/twitter-entity-sentiment-analysis\n",
        "\n",
        " !unzip twitter-entity-sentiment-analysis.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l8QfjykKlPqG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd             # Untuk manipulasi dan analisis data I/O dataframe (ex read dataset)\n",
        "import numpy as np              # Untuk operasi numerik dan komputasi\n",
        "import matplotlib.pyplot as plt # Untuk visualisasi grafik dan plot\n",
        "import seaborn as sns           # Dibangun di atas plt, menyediakan visualisasi yang lebih menarik\n",
        "import warnings                 # Handling peringatan yang ditampilkan\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSRpXPnu0vlL"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('twitter_training.csv', error_bad_lines=False)\n",
        "train_df.columns = ['TweetID', 'Entity', 'Sentiment', 'Content']\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY9sAdK90v5u"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIs_eN550xrp"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('twitter_validation.csv')\n",
        "test_df.columns = ['TweetID', 'Entity', 'Sentiment', 'Content']\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge8g7E_h0zA7"
      },
      "outputs": [],
      "source": [
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUEymeM204-J"
      },
      "outputs": [],
      "source": [
        "nan_row = train_df[train_df['Content'].isna()]\n",
        "nan_row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnXAOu0kgaMH"
      },
      "outputs": [],
      "source": [
        "train_df.drop(nan_row.index, axis=0, inplace=True)\n",
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O87FwWvgduz"
      },
      "outputs": [],
      "source": [
        "train_duplicate = train_df[train_df.duplicated(subset=['Entity', 'Sentiment', 'Content'], keep=False)]\n",
        "train_duplicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXNVKGFAgjvm"
      },
      "outputs": [],
      "source": [
        "test_duplicate = test_df[test_df.duplicated(subset=['Entity', 'Sentiment', 'Content'], keep=False)]\n",
        "test_duplicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObFwcVCIgocX"
      },
      "outputs": [],
      "source": [
        "train_df.drop(train_duplicate.index, axis=0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YtZ3m5v1Rgo"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-CdKJWY3hiA"
      },
      "outputs": [],
      "source": [
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2lNL0n7ss5J"
      },
      "outputs": [],
      "source": [
        "sentiment_counts = train_df['Sentiment'].value_counts()\n",
        "\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
        "\n",
        "plt.title('Sentiment Distribution Train')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3cFyXmW3lq-"
      },
      "source": [
        "## Data Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lviXAtBJ1Vfr"
      },
      "outputs": [],
      "source": [
        "train_df.drop('TweetID', axis=1, inplace=True)\n",
        "# train_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv2ibhwFs6KJ"
      },
      "source": [
        "### Emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMn7KM8o3KUB"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "!pip install emot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43XyGM7qsq2b"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import emoji\n",
        "from tqdm import tqdm\n",
        "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwDKLHyFtYHW"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxL0QDZM32A5"
      },
      "outputs": [],
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "def convert_emojis(text):\n",
        "    text_list = word_tokenize(text)\n",
        "    new_list = [UNICODE_EMOJI[word.lower()] if word.lower() in UNICODE_EMOJI.keys() else word for word in text_list]\n",
        "    return  \" \".join(new_list)\n",
        "\n",
        "def convert_emoticons(text):\n",
        "    text_list = word_tokenize(text)\n",
        "    new_list = [EMOTICONS_EMO[word.lower()] if word.lower() in EMOTICONS_EMO.keys() else word for word in text_list]\n",
        "    return  \" \".join(new_list)\n",
        "\n",
        "\n",
        "# Apply transformations with progress bars\n",
        "train_df['emo_content'] = train_df['Content'].progress_apply(convert_emojis)\n",
        "train_df['emo_cotent'] = train_df['Content'].progress_apply(convert_emoticons)\n",
        "\n",
        "test_df['emo_content'] = test_df['Content'].progress_apply(convert_emojis)\n",
        "test_df['emo_cotent'] = test_df['Content'].progress_apply(convert_emoticons)\n",
        "\n",
        "train_df['emo_content']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNbmV4nIteB0"
      },
      "outputs": [],
      "source": [
        "abbreviations = {\n",
        "    \"$\" : \" dollar \",\n",
        "    \"€\" : \" euro \",\n",
        "    \"4ao\" : \"for adults only\",\n",
        "    \"a.m\" : \"before midday\",\n",
        "    \"a3\" : \"anytime anywhere anyplace\",\n",
        "    \"aamof\" : \"as a matter of fact\",\n",
        "    \"acct\" : \"account\",\n",
        "    \"adih\" : \"another day in hell\",\n",
        "    \"afaic\" : \"as far as i am concerned\",\n",
        "    \"afaict\" : \"as far as i can tell\",\n",
        "    \"afaik\" : \"as far as i know\",\n",
        "    \"afair\" : \"as far as i remember\",\n",
        "    \"afk\" : \"away from keyboard\",\n",
        "    \"app\" : \"application\",\n",
        "    \"approx\" : \"approximately\",\n",
        "    \"apps\" : \"applications\",\n",
        "    \"asap\" : \"as soon as possible\",\n",
        "    \"asl\" : \"age, sex, location\",\n",
        "    \"atk\" : \"at the keyboard\",\n",
        "    \"ave.\" : \"avenue\",\n",
        "    \"aymm\" : \"are you my mother\",\n",
        "    \"ayor\" : \"at your own risk\",\n",
        "    \"b&b\" : \"bed and breakfast\",\n",
        "    \"b+b\" : \"bed and breakfast\",\n",
        "    \"b.c\" : \"before christ\",\n",
        "    \"b2b\" : \"business to business\",\n",
        "    \"b2c\" : \"business to customer\",\n",
        "    \"b4\" : \"before\",\n",
        "    \"b4n\" : \"bye for now\",\n",
        "    \"b@u\" : \"back at you\",\n",
        "    \"bae\" : \"before anyone else\",\n",
        "    \"bak\" : \"back at keyboard\",\n",
        "    \"bbbg\" : \"bye bye be good\",\n",
        "    \"bbc\" : \"british broadcasting corporation\",\n",
        "    \"bbias\" : \"be back in a second\",\n",
        "    \"bbl\" : \"be back later\",\n",
        "    \"bbs\" : \"be back soon\",\n",
        "    \"be4\" : \"before\",\n",
        "    \"bfn\" : \"bye for now\",\n",
        "    \"blvd\" : \"boulevard\",\n",
        "    \"bout\" : \"about\",\n",
        "    \"brb\" : \"be right back\",\n",
        "    \"bros\" : \"brothers\",\n",
        "    \"brt\" : \"be right there\",\n",
        "    \"bsaaw\" : \"big smile and a wink\",\n",
        "    \"btw\" : \"by the way\",\n",
        "    \"bwl\" : \"bursting with laughter\",\n",
        "    \"c/o\" : \"care of\",\n",
        "    \"cet\" : \"central european time\",\n",
        "    \"cf\" : \"compare\",\n",
        "    \"cia\" : \"central intelligence agency\",\n",
        "    \"csl\" : \"can not stop laughing\",\n",
        "    \"cu\" : \"see you\",\n",
        "    \"cul8r\" : \"see you later\",\n",
        "    \"cv\" : \"curriculum vitae\",\n",
        "    \"cwot\" : \"complete waste of time\",\n",
        "    \"cya\" : \"see you\",\n",
        "    \"cyt\" : \"see you tomorrow\",\n",
        "    \"dae\" : \"does anyone else\",\n",
        "    \"dbmib\" : \"do not bother me i am busy\",\n",
        "    \"diy\" : \"do it yourself\",\n",
        "    \"dm\" : \"direct message\",\n",
        "    \"dwh\" : \"during work hours\",\n",
        "    \"e123\" : \"easy as one two three\",\n",
        "    \"eet\" : \"eastern european time\",\n",
        "    \"eg\" : \"example\",\n",
        "    \"embm\" : \"early morning business meeting\",\n",
        "    \"encl\" : \"enclosed\",\n",
        "    \"encl.\" : \"enclosed\",\n",
        "    \"etc\" : \"and so on\",\n",
        "    \"faq\" : \"frequently asked questions\",\n",
        "    \"fawc\" : \"for anyone who cares\",\n",
        "    \"fb\" : \"facebook\",\n",
        "    \"fc\" : \"fingers crossed\",\n",
        "    \"fig\" : \"figure\",\n",
        "    \"fimh\" : \"forever in my heart\",\n",
        "    \"ft.\" : \"feet\",\n",
        "    \"ft\" : \"featuring\",\n",
        "    \"ftl\" : \"for the loss\",\n",
        "    \"ftw\" : \"for the win\",\n",
        "    \"fwiw\" : \"for what it is worth\",\n",
        "    \"fyi\" : \"for your information\",\n",
        "    \"g9\" : \"genius\",\n",
        "    \"gahoy\" : \"get a hold of yourself\",\n",
        "    \"gal\" : \"get a life\",\n",
        "    \"gcse\" : \"general certificate of secondary education\",\n",
        "    \"gfn\" : \"gone for now\",\n",
        "    \"gg\" : \"good game\",\n",
        "    \"gl\" : \"good luck\",\n",
        "    \"glhf\" : \"good luck have fun\",\n",
        "    \"gmt\" : \"greenwich mean time\",\n",
        "    \"gmta\" : \"great minds think alike\",\n",
        "    \"gn\" : \"good night\",\n",
        "    \"g.o.a.t\" : \"greatest of all time\",\n",
        "    \"goat\" : \"greatest of all time\",\n",
        "    \"goi\" : \"get over it\",\n",
        "    \"gps\" : \"global positioning system\",\n",
        "    \"gr8\" : \"great\",\n",
        "    \"gratz\" : \"congratulations\",\n",
        "    \"gyal\" : \"girl\",\n",
        "    \"h&c\" : \"hot and cold\",\n",
        "    \"hp\" : \"horsepower\",\n",
        "    \"hr\" : \"hour\",\n",
        "    \"hrh\" : \"his royal highness\",\n",
        "    \"ht\" : \"height\",\n",
        "    \"ibrb\" : \"i will be right back\",\n",
        "    \"ic\" : \"i see\",\n",
        "    \"icq\" : \"i seek you\",\n",
        "    \"icymi\" : \"in case you missed it\",\n",
        "    \"idc\" : \"i do not care\",\n",
        "    \"idgadf\" : \"i do not give a damn fuck\",\n",
        "    \"idgaf\" : \"i do not give a fuck\",\n",
        "    \"idk\" : \"i do not know\",\n",
        "    \"ie\" : \"that is\",\n",
        "    \"i.e\" : \"that is\",\n",
        "    \"ifyp\" : \"i feel your pain\",\n",
        "    \"IG\" : \"instagram\",\n",
        "    \"iirc\" : \"if i remember correctly\",\n",
        "    \"ilu\" : \"i love you\",\n",
        "    \"ily\" : \"i love you\",\n",
        "    \"imho\" : \"in my humble opinion\",\n",
        "    \"imo\" : \"in my opinion\",\n",
        "    \"imu\" : \"i miss you\",\n",
        "    \"iow\" : \"in other words\",\n",
        "    \"irl\" : \"in real life\",\n",
        "    \"j4f\" : \"just for fun\",\n",
        "    \"jic\" : \"just in case\",\n",
        "    \"jk\" : \"just kidding\",\n",
        "    \"jsyk\" : \"just so you know\",\n",
        "    \"l8r\" : \"later\",\n",
        "    \"lb\" : \"pound\",\n",
        "    \"lbs\" : \"pounds\",\n",
        "    \"ldr\" : \"long distance relationship\",\n",
        "    \"lmao\" : \"laugh my ass off\",\n",
        "    \"lmfao\" : \"laugh my fucking ass off\",\n",
        "    \"lol\" : \"laughing out loud\",\n",
        "    \"ltd\" : \"limited\",\n",
        "    \"ltns\" : \"long time no see\",\n",
        "    \"m8\" : \"mate\",\n",
        "    \"mf\" : \"motherfucker\",\n",
        "    \"mfs\" : \"motherfuckers\",\n",
        "    \"mfw\" : \"my face when\",\n",
        "    \"mofo\" : \"motherfucker\",\n",
        "    \"mph\" : \"miles per hour\",\n",
        "    \"mr\" : \"mister\",\n",
        "    \"mrw\" : \"my reaction when\",\n",
        "    \"ms\" : \"miss\",\n",
        "    \"mte\" : \"my thoughts exactly\",\n",
        "    \"nagi\" : \"not a good idea\",\n",
        "    \"nbc\" : \"national broadcasting company\",\n",
        "    \"nbd\" : \"not big deal\",\n",
        "    \"nfs\" : \"not for sale\",\n",
        "    \"ngl\" : \"not going to lie\",\n",
        "    \"nhs\" : \"national health service\",\n",
        "    \"nrn\" : \"no reply necessary\",\n",
        "    \"nsfl\" : \"not safe for life\",\n",
        "    \"nsfw\" : \"not safe for work\",\n",
        "    \"nth\" : \"nice to have\",\n",
        "    \"nvr\" : \"never\",\n",
        "    \"nyc\" : \"new york city\",\n",
        "    \"oc\" : \"original content\",\n",
        "    \"og\" : \"original\",\n",
        "    \"ohp\" : \"overhead projector\",\n",
        "    \"oic\" : \"oh i see\",\n",
        "    \"omdb\" : \"over my dead body\",\n",
        "    \"omg\" : \"oh my god\",\n",
        "    \"omw\" : \"on my way\",\n",
        "    \"p.a\" : \"per annum\",\n",
        "    \"p.m\" : \"after midday\",\n",
        "    \"pm\" : \"prime minister\",\n",
        "    \"poc\" : \"people of color\",\n",
        "    \"pov\" : \"point of view\",\n",
        "    \"pp\" : \"pages\",\n",
        "    \"ppl\" : \"people\",\n",
        "    \"prw\" : \"parents are watching\",\n",
        "    \"ps\" : \"postscript\",\n",
        "    \"pt\" : \"point\",\n",
        "    \"ptb\" : \"please text back\",\n",
        "    \"pto\" : \"please turn over\",\n",
        "    \"qpsa\" : \"what happens\",\n",
        "    \"ratchet\" : \"rude\",\n",
        "    \"rbtl\" : \"read between the lines\",\n",
        "    \"rlrt\" : \"real life retweet\",\n",
        "    \"rofl\" : \"rolling on the floor laughing\",\n",
        "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\" : \"retweet\",\n",
        "    \"ruok\" : \"are you ok\",\n",
        "    \"sfw\" : \"safe for work\",\n",
        "    \"sk8\" : \"skate\",\n",
        "    \"smh\" : \"shake my head\",\n",
        "    \"sq\" : \"square\",\n",
        "    \"srsly\" : \"seriously\",\n",
        "    \"ssdd\" : \"same stuff different day\",\n",
        "    \"tbh\" : \"to be honest\",\n",
        "    \"tbs\" : \"tablespooful\",\n",
        "    \"tbsp\" : \"tablespooful\",\n",
        "    \"tfw\" : \"that feeling when\",\n",
        "    \"thks\" : \"thank you\",\n",
        "    \"tho\" : \"though\",\n",
        "    \"thx\" : \"thank you\",\n",
        "    \"tia\" : \"thanks in advance\",\n",
        "    \"til\" : \"today i learned\",\n",
        "    \"tl;dr\" : \"too long i did not read\",\n",
        "    \"tldr\" : \"too long i did not read\",\n",
        "    \"tmb\" : \"tweet me back\",\n",
        "    \"tntl\" : \"trying not to laugh\",\n",
        "    \"ttyl\" : \"talk to you later\",\n",
        "    \"u\" : \"you\",\n",
        "    \"u2\" : \"you too\",\n",
        "    \"u4e\" : \"yours for ever\",\n",
        "    \"utc\" : \"coordinated universal time\",\n",
        "    \"w/\" : \"with\",\n",
        "    \"w/o\" : \"without\",\n",
        "    \"w8\" : \"wait\",\n",
        "    \"wassup\" : \"what is up\",\n",
        "    \"wb\" : \"welcome back\",\n",
        "    \"wtf\" : \"what the fuck\",\n",
        "    \"wtg\" : \"way to go\",\n",
        "    \"wtpa\" : \"where the party at\",\n",
        "    \"wuf\" : \"where are you from\",\n",
        "    \"wuzup\" : \"what is up\",\n",
        "    \"wywh\" : \"wish you were here\",\n",
        "    \"yd\" : \"yard\",\n",
        "    \"ygtr\" : \"you got that right\",\n",
        "    \"ynk\" : \"you never know\",\n",
        "    \"zzz\" : \"sleeping bored and tired\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjCmukja4Q2w"
      },
      "outputs": [],
      "source": [
        "def convert_abbrev(word):\n",
        "    word_list = word_tokenize(word)\n",
        "    new_list = [abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word for word in word_list]\n",
        "    return \" \".join(new_list)\n",
        "\n",
        "train_df['abbrev_content'] = train_df['emo_content'].apply(convert_abbrev)\n",
        "test_df['abbrev_content'] = test_df['emo_content'].apply(convert_abbrev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiOuP6pH47eI"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "def remove_extra(input_string):\n",
        "    #Remove link\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text_without_link = re.sub(url_pattern, '', input_string)\n",
        "\n",
        "    # Remove \"@\" symbols\n",
        "    text_without_ad = text_without_link.replace('@', '')\n",
        "\n",
        "    #Remove Hastag\n",
        "    hashtag_pattern = re.compile(r'#\\w+')\n",
        "    text_without_hastag = re.sub(hashtag_pattern, '', text_without_ad)\n",
        "\n",
        "    punctuation_except_comma_period = '[%s]' % re.escape(''.join(c for c in string.punctuation))\n",
        "    text_without_special_char = re.sub(punctuation_except_comma_period, '', text_without_hastag)\n",
        "\n",
        "    text_without_extra_space = re.sub(' +', ' ', text_without_special_char)\n",
        "    return text_without_extra_space\n",
        "\n",
        "train_df['delete_extra'] = train_df['abbrev_content'].apply(remove_extra)\n",
        "test_df['delete_extra'] = test_df['abbrev_content'].apply(remove_extra)\n",
        "train_df['delete_extra']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u6B2X-U4197"
      },
      "outputs": [],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqfsr_upttkS"
      },
      "outputs": [],
      "source": [
        "import contractions\n",
        "def expand_shortcut(text) :\n",
        "    expanded_words = []\n",
        "    for word in text.split():\n",
        "        expanded_words.append(contractions.fix(word))\n",
        "\n",
        "    return ' '.join(expanded_words)\n",
        "\n",
        "train_df['expanded_content'] = train_df['delete_extra'].apply(expand_shortcut)\n",
        "test_df['expanded_content'] = test_df['delete_extra'].apply(expand_shortcut)\n",
        "\n",
        "train_df['expanded_content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpKfDLlXtwXZ"
      },
      "outputs": [],
      "source": [
        "train_df['lower_content'] = train_df['expanded_content'].apply(lambda x: x.lower())\n",
        "test_df['lower_content'] = test_df['expanded_content'].apply(lambda x: x.lower())\n",
        "\n",
        "train_df['lower_content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKIak-optyqd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQyVdRmEt08L"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "train_df['tokenized_content'] = train_df['lower_content'].apply(word_tokenize)\n",
        "test_df['tokenized_content'] = test_df['lower_content'].apply(word_tokenize)\n",
        "\n",
        "train_df['tokenized_content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vtQHDrrt2_g"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    nltk.data.find('wordnet.zip')\n",
        "except:\n",
        "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
        "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
        "    subprocess.run(command.split())\n",
        "    nltk.data.path.append('/kaggle/working/')\n",
        "\n",
        "# Now you can import the NLTK resources as usual\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjMOoGWVt40G"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return [lemmatizer.lemmatize(word) for word in text]\n",
        "train_df['lemma_content'] = train_df['tokenized_content'].apply(lemmatize_words)\n",
        "test_df['lemma_content'] = test_df['tokenized_content'].apply(lemmatize_words)\n",
        "\n",
        "train_df['lemma_content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5joT8dnzt7Of"
      },
      "outputs": [],
      "source": [
        "train_df['lemma_content'][11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQMHIMPRt_v8"
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt search enchant\n",
        "!apt install -y enchant-2\n",
        "!apt install -qq enchant -y\n",
        "!pip install pyenchant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08NXGQZFuBvu"
      },
      "outputs": [],
      "source": [
        "import enchant\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "def is_valid_word(word):\n",
        "    dictionary = enchant.Dict(\"en_US\")\n",
        "    return dictionary.check(word)\n",
        "\n",
        "def check_en_word(li):\n",
        "    new_list = [w for w in li if is_valid_word(w)]\n",
        "    return new_list\n",
        "\n",
        "train_df['en_content'] = train_df['lemma_content'].progress_apply(check_en_word)\n",
        "test_df['en_content'] = test_df['lemma_content'].progress_apply(check_en_word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwLPbL7LuKYa"
      },
      "outputs": [],
      "source": [
        "train_df['en_content']\n",
        "# train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCoEu3s-uwRX"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQFjOyHuuyKn"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    filtered_sentence = [w for w in text if not w.lower() in stop_words]\n",
        "    return filtered_sentence\n",
        "\n",
        "train_df['remove_stopwords'] = train_df['en_content'].apply(remove_stopwords)\n",
        "test_df['remove_stopwords'] = test_df['en_content'].apply(remove_stopwords)\n",
        "\n",
        "train_df['remove_stopwords']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMmrLyX2u0oZ"
      },
      "outputs": [],
      "source": [
        "# train_df[['Sentiment', 'remove_stopwords']]\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PADvwGE0u4nO"
      },
      "outputs": [],
      "source": [
        "duplicated = train_df[train_df.duplicated(subset=['remove_stopwords'], keep=False)]\n",
        "duplicated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHR6FdAou60h"
      },
      "outputs": [],
      "source": [
        "train_df.drop(duplicated.index, axis=0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX7tN5JUDZQO"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoR4WL5bu9XS"
      },
      "outputs": [],
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "def combine_list(entity, lst):\n",
        "    str_list = ' '.join(lst)\n",
        "\n",
        "    return f'{entity} [SEP] {str_list}'\n",
        "\n",
        "train_df.loc[:,'final_result'] = train_df.apply(lambda x: combine_list(x['Entity'], x['en_content']), axis=1)\n",
        "test_df.loc[:,'final_result'] = test_df.apply(lambda x: combine_list(x['Entity'], x['en_content']), axis=1)\n",
        "test_df['final_result']\n",
        "train_df['final_result']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyzFAdj75Y8p"
      },
      "source": [
        "##Explanatory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V5UY4q25Idh"
      },
      "outputs": [],
      "source": [
        "sentiment_counts = train_df['Sentiment'].value_counts()\n",
        "\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
        "\n",
        "plt.title('Sentiment Distribution Train')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPCdQ1NH8Ark"
      },
      "source": [
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiMQ8Hpq5kfF"
      },
      "outputs": [],
      "source": [
        "#Word Count\n",
        "train_df['Word_Count'] = train_df.apply(lambda x: len(str(x['Content']).split()), axis=1)\n",
        "train_df['Word_Count'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqHuZ58S77-u"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(21,9))\n",
        "\n",
        "n = 1\n",
        "\n",
        "for i in range(len(train_df['Sentiment'].unique())):\n",
        "  plt.subplot(1, len(train_df['Sentiment'].unique()), n)\n",
        "  n += 1\n",
        "  sns.histplot(x = \"Word_Count\", data=train_df[train_df['Sentiment'] == train_df['Sentiment'].unique()[i]])\n",
        "  plt.title(f'Word Count {train_df[\"Sentiment\"].unique()[i]}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFAX7wLWAnDK"
      },
      "source": [
        "##Analysis Number Of Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Hj60ZL5BMs9"
      },
      "outputs": [],
      "source": [
        "# #NEGATIVE\n",
        "\n",
        "# neg_train_df = train_df[train_df[\"Sentiment\"] == \"Negative\"][\"Content\"]\n",
        "\n",
        "# neg_train_words = ' '.join(neg_train_df)\n",
        "# neg_train_words = re.sub(r'[^\\w\\s]', '', neg_train_words.lower())\n",
        "\n",
        "\n",
        "# neg_train_words = neg_train_words.split()\n",
        "\n",
        "# word_freq = Counter(word for word in neg_train_words if word not in STOPWORDS)\n",
        "# word_freq.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIRJurBPDiJ3"
      },
      "outputs": [],
      "source": [
        "# #NEUTRAL\n",
        "\n",
        "# neu_train_df = train_df[train_df[\"Sentiment\"] == \"Neutral\"][\"Content\"]\n",
        "\n",
        "# neu_train_words = ' '.join(neu_train_df)\n",
        "# neu_train_words = re.sub(r'[^\\w\\s]', '', neu_train_words.lower())\n",
        "\n",
        "# neu_train_words = neu_train_words.split()\n",
        "\n",
        "# word_freq = Counter(word for word in neu_train_words if word not in STOPWORDS)\n",
        "# word_freq.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Ui700dDM5R"
      },
      "outputs": [],
      "source": [
        "# #IRRELEVANT\n",
        "\n",
        "# irr_train_df = train_df[train_df[\"Sentiment\"] == \"Irrelevant\"][\"Content\"]\n",
        "\n",
        "# irr_train_words = ' '.join(irr_train_df)\n",
        "# irr_train_words = re.sub(r'[^\\w\\s]', '', irr_train_words.lower())\n",
        "\n",
        "# irr_train_words = irr_train_words.split()\n",
        "\n",
        "# word_freq = Counter(word for word in irr_train_words if word not in STOPWORDS)\n",
        "# word_freq.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbKU0dIqQHjX"
      },
      "source": [
        "##Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRajB33zQQqK"
      },
      "source": [
        "### XLNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XuuBRgLvQMZy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.36.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\andrian21\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\andrian21\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RJdqdw0-DWGY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from transformers import TFXLNetModel, XLNetTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OqVnIRC4CtFi"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Entity</th>\n",
              "      <th>Content</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Borderlands</td>\n",
              "      <td>['i', 'am', 'coming', 'to', 'the', 'border', '...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Borderlands</td>\n",
              "      <td>['i', 'am', 'getting', 'on', 'borderland', 'an...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Borderlands</td>\n",
              "      <td>['i', 'am', 'coming', 'on', 'borderland', 'and...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Borderlands</td>\n",
              "      <td>['i', 'am', 'getting', 'on', 'borderland', '2'...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Borderlands</td>\n",
              "      <td>['i', 'am', 'getting', 'into', 'borderland', '...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51576</th>\n",
              "      <td>Nvidia</td>\n",
              "      <td>['my', 'be', 'no', 'highlight', 'picture', 're...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51577</th>\n",
              "      <td>Nvidia</td>\n",
              "      <td>['just', 'realized', 'that', 'my', 'mac', 'win...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51578</th>\n",
              "      <td>Nvidia</td>\n",
              "      <td>['just', 'realized', 'the', 'window', 'partiti...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51579</th>\n",
              "      <td>Nvidia</td>\n",
              "      <td>['just', 'realized', 'between', 'the', 'window...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51580</th>\n",
              "      <td>Nvidia</td>\n",
              "      <td>['just', 'like', 'the', 'window', 'partition',...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>51581 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Entity                                            Content  \\\n",
              "0      Borderlands  ['i', 'am', 'coming', 'to', 'the', 'border', '...   \n",
              "1      Borderlands  ['i', 'am', 'getting', 'on', 'borderland', 'an...   \n",
              "2      Borderlands  ['i', 'am', 'coming', 'on', 'borderland', 'and...   \n",
              "3      Borderlands  ['i', 'am', 'getting', 'on', 'borderland', '2'...   \n",
              "4      Borderlands  ['i', 'am', 'getting', 'into', 'borderland', '...   \n",
              "...            ...                                                ...   \n",
              "51576       Nvidia  ['my', 'be', 'no', 'highlight', 'picture', 're...   \n",
              "51577       Nvidia  ['just', 'realized', 'that', 'my', 'mac', 'win...   \n",
              "51578       Nvidia  ['just', 'realized', 'the', 'window', 'partiti...   \n",
              "51579       Nvidia  ['just', 'realized', 'between', 'the', 'window...   \n",
              "51580       Nvidia  ['just', 'like', 'the', 'window', 'partition',...   \n",
              "\n",
              "      Sentiment  \n",
              "0      Positive  \n",
              "1      Positive  \n",
              "2      Positive  \n",
              "3      Positive  \n",
              "4      Positive  \n",
              "...         ...  \n",
              "51576  Positive  \n",
              "51577  Positive  \n",
              "51578  Positive  \n",
              "51579  Positive  \n",
              "51580  Positive  \n",
              "\n",
              "[51581 rows x 3 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load and preprocess the data\n",
        "train_df = pd.read_csv('processed_train_df.csv')\n",
        "train_df = train_df[[\"Entity\", \"en_content\", \"Sentiment\"]]\n",
        "\n",
        "train_df.columns = [\"Entity\", \"Content\", \"Sentiment\"]\n",
        "\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Entity</th>\n",
              "      <th>Content</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Amazon</td>\n",
              "      <td>['broadcasting', 'corporation', 'news', 'amazo...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Microsoft</td>\n",
              "      <td>['why', 'do', 'i', 'pay', 'for', 'word', 'when...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CS-GO</td>\n",
              "      <td>['matchmaking', 'is', 'so', 'full', 'of', 'clo...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Google</td>\n",
              "      <td>['now', 'the', 'president', 'is', 'slapping', ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>FIFA</td>\n",
              "      <td>['hi', 'i', 'had', 'in', 'my', 'cellar', 'for'...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>GrandTheftAuto(GTA)</td>\n",
              "      <td>['is', 'the', 'art', 'and', 'culture', 'capita...</td>\n",
              "      <td>Irrelevant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>CS-GO</td>\n",
              "      <td>['this', 'is', 'actually', 'a', 'good', 'move'...</td>\n",
              "      <td>Irrelevant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Borderlands</td>\n",
              "      <td>['today', 'sucked', 'so', 'it', 's', 'time', '...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Microsoft</td>\n",
              "      <td>['bought', 'a', 'fraction', 'of', 'today', 'sm...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>johnson&amp;johnson</td>\n",
              "      <td>['to', 'stop', 'selling', 'talc', 'baby', 'pow...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>999 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Entity                                            Content  \\\n",
              "0                 Amazon  ['broadcasting', 'corporation', 'news', 'amazo...   \n",
              "1              Microsoft  ['why', 'do', 'i', 'pay', 'for', 'word', 'when...   \n",
              "2                  CS-GO  ['matchmaking', 'is', 'so', 'full', 'of', 'clo...   \n",
              "3                 Google  ['now', 'the', 'president', 'is', 'slapping', ...   \n",
              "4                   FIFA  ['hi', 'i', 'had', 'in', 'my', 'cellar', 'for'...   \n",
              "..                   ...                                                ...   \n",
              "994  GrandTheftAuto(GTA)  ['is', 'the', 'art', 'and', 'culture', 'capita...   \n",
              "995                CS-GO  ['this', 'is', 'actually', 'a', 'good', 'move'...   \n",
              "996          Borderlands  ['today', 'sucked', 'so', 'it', 's', 'time', '...   \n",
              "997            Microsoft  ['bought', 'a', 'fraction', 'of', 'today', 'sm...   \n",
              "998      johnson&johnson  ['to', 'stop', 'selling', 'talc', 'baby', 'pow...   \n",
              "\n",
              "      Sentiment  \n",
              "0       Neutral  \n",
              "1      Negative  \n",
              "2      Negative  \n",
              "3       Neutral  \n",
              "4      Negative  \n",
              "..          ...  \n",
              "994  Irrelevant  \n",
              "995  Irrelevant  \n",
              "996    Positive  \n",
              "997    Positive  \n",
              "998     Neutral  \n",
              "\n",
              "[999 rows x 3 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load and preprocess the data\n",
        "test_df = pd.read_csv('processed_test_df.csv')\n",
        "test_df = test_df[[\"Entity\", \"en_content\", \"Sentiment\"]]\n",
        "\n",
        "test_df.columns = [\"Entity\", \"Content\", \"Sentiment\"]\n",
        "\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "def combine_list(entity, lst):\n",
        "    actual_list = literal_eval(lst)\n",
        "    str_list = ' '.join(actual_list)\n",
        "\n",
        "    return f'{entity.lower()} : {str_list}'\n",
        "\n",
        "train_df.loc[:, 'Content'] = train_df.apply(lambda x: combine_list(x['Entity'], x['Content']), axis=1)\n",
        "test_df.loc[:, 'Content'] = test_df.apply(lambda x: combine_list(x['Entity'], x['Content']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd2nJgIIQGY-",
        "outputId": "1ed11bd6-1d79-4cb3-dda6-3dca54682036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetModel: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFXLNetModel were initialized from the model checkpoint at xlnet-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:From c:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:5575: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:From c:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "3224/3224 [==============================] - ETA: 0s - loss: 1.1801 - accuracy: 0.5041\n",
            "Epoch 1: val_loss improved from inf to 1.14382, saving model to xlnet.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3224/3224 [==============================] - 9599s 3s/step - loss: 1.1801 - accuracy: 0.5041 - val_loss: 1.1438 - val_accuracy: 0.5746\n",
            "Epoch 2/10\n",
            "3224/3224 [==============================] - ETA: 0s - loss: 1.0015 - accuracy: 0.5920\n",
            "Epoch 2: val_loss improved from 1.14382 to 1.01123, saving model to xlnet.h5\n",
            "3224/3224 [==============================] - 9727s 3s/step - loss: 1.0015 - accuracy: 0.5920 - val_loss: 1.0112 - val_accuracy: 0.6166\n",
            "Epoch 3/10\n",
            "2721/3224 [========================>.....] - ETA: 12:37:59 - loss: 0.9110 - accuracy: 0.6344"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping and model checkpoint\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     99\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(val_dataset\u001b[38;5;241m.\u001b[39mbatch(batch_size))\n",
            "File \u001b[1;32mc:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32mc:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\andrian21\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "model = TFXLNetModel.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "# Load and preprocess the data\n",
        "# train_df = train_df[[\"Entity\", \"en_content\", \"Sentiment\"]]\n",
        "\n",
        "# train_df.columns = [\"Entity\", \"Content\", \"Sentiment\"]\n",
        "\n",
        "# data = train_df\n",
        "\n",
        "# data['Sentiment_label'] = pd.Categorical(data['Sentiment'])\n",
        "# data['Sentiment'] = data['Sentiment_label'].cat.codes\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# data_train, data_val = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# # Load and preprocess the data\n",
        "data_train = train_df[['Content', 'Sentiment']]\n",
        "data_train['Sentiment_label'] = pd.Categorical(data_train['Sentiment'])\n",
        "data_train['Sentiment'] = data_train['Sentiment_label'].cat.codes\n",
        "\n",
        "# # Split the data into training and validation sets\n",
        "data_val = test_df[['Content', 'Sentiment']]\n",
        "data_val['Sentiment_label'] = pd.Categorical(data_val['Sentiment'])\n",
        "data_val['Sentiment'] = data_val['Sentiment_label'].cat.codes\n",
        "\n",
        "# Extract the training and validation texts and labels\n",
        "train_texts = data_train['Content'].tolist()\n",
        "train_labels = data_train['Sentiment'].tolist()\n",
        "val_texts = data_val['Content'].tolist()\n",
        "val_labels = data_val['Sentiment'].tolist()\n",
        "\n",
        "# Extract the training and testing texts and labels\n",
        "train_texts = data_train['Content'].tolist()\n",
        "train_labels = data_train['Sentiment'].tolist()\n",
        "val_texts = data_val['Content'].tolist()\n",
        "val_labels = data_val['Sentiment'].tolist()\n",
        "\n",
        "# Tokenize the texts\n",
        "# Modified code starts here\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=False, max_length=64)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=False, max_length=64)\n",
        "\n",
        "train_input_ids = pad_sequences(train_encodings['input_ids'], maxlen=64, padding='post')\n",
        "train_attention_mask = pad_sequences(train_encodings['attention_mask'], maxlen=64, padding='post')\n",
        "\n",
        "val_input_ids = pad_sequences(val_encodings['input_ids'], maxlen=64, padding='post')\n",
        "val_attention_mask = pad_sequences(val_encodings['attention_mask'], maxlen=64, padding='post')\n",
        "# Modified code ends here\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "num_labels = len(data_train['Sentiment_label'].cat.categories)\n",
        "train_labels_encoded = tf.one_hot(train_labels, num_labels)\n",
        "val_labels_encoded = tf.one_hot(val_labels, num_labels)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_input_ids, 'attention_mask': train_attention_mask}, train_labels_encoded))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_input_ids, 'attention_mask': val_attention_mask}, val_labels_encoded))\n",
        "\n",
        "# Define the model architecture\n",
        "input_ids = tf.keras.layers.Input(shape=(64,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.layers.Input(shape=(64,), dtype=tf.int32, name='attention_mask')\n",
        "output = model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "output = tf.keras.layers.Dense(num_labels, activation='softmax')(output[:, 0, :])  # Pooling the output\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "# Compile and train the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Define the ModelCheckpoint callback to save the best weights\n",
        "checkpoint_filepath = 'xlnet.h5'\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Use smaller batch size\n",
        "batch_size = 16\n",
        "epochs = 10\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "# Train the model with early stopping and model checkpoint\n",
        "history = model.fit(\n",
        "    train_dataset.batch(batch_size),\n",
        "    epochs=epochs,\n",
        "    validation_data=val_dataset.batch(batch_size),\n",
        "    callbacks=[model_checkpoint]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(val_dataset.batch(batch_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHmaiKhC8i-X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "# Load and preprocess the data\n",
        "data_val = test_df[['Content', 'Sentiment']]\n",
        "data_val['Sentiment_label'] = pd.Categorical(data_val['Sentiment'])\n",
        "data_val['Sentiment'] = data_val['Sentiment_label'].cat.codes\n",
        "\n",
        "test_texts = data_val['Content'].tolist()\n",
        "test_labels = data_val['Sentiment'].tolist()\n",
        "\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=24)\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "num_labels = len(data_val['Sentiment_label'].cat.categories)\n",
        "val_labels_encoded = tf.one_hot(val_labels, num_labels)\n",
        "test_labels_encoded = tf.one_hot(test_labels, num_labels)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels_encoded))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels_encoded))\n",
        "\n",
        "val_predictions = model.predict(val_dataset.batch(64))\n",
        "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
        "test_predictions = model.predict(test_dataset.batch(64))\n",
        "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Convert the predicted labels to their original sentiment categories\n",
        "val_predicted_sentiments = data['Sentiment_label'].cat.categories[val_predicted_labels]\n",
        "test_predicted_sentiments = data['Sentiment_label'].cat.categories[test_predicted_labels]\n",
        "\n",
        "# Convert the true labels to their original sentiment categories\n",
        "val_true_labels = data_val['Sentiment_label']\n",
        "test_true_labels = data_val['Sentiment_label']\n",
        "\n",
        "# Calculate the classification report for the valing set\n",
        "val_classification_rep = classification_report(val_true_labels, val_predicted_sentiments)\n",
        "print(\"Validation Set - Classification Report:\\n\", val_classification_rep)\n",
        "\n",
        "# Generate the confusion matrix for the valdation set\n",
        "val_confusion_mat = confusion_matrix(val_true_labels, val_predicted_sentiments)\n",
        "\n",
        "# Get the unique labels/categories from the true labels\n",
        "labels = np.unique(val_true_labels)\n",
        "\n",
        "# Plot the confusion matrix for the valing set\n",
        "val_display = ConfusionMatrixDisplay(confusion_matrix=val_confusion_mat, display_labels=labels)\n",
        "val_display.plot(cmap='Blues')\n",
        "plt.title(\"valing Set - Confusion Matrix\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# Calculate the classification report for the test set\n",
        "test_classification_rep = classification_report(test_true_labels, test_predicted_sentiments)\n",
        "print(\"Test Set - Classification Report:\\n\", test_classification_rep)\n",
        "\n",
        "# Generate the confusion matrix for the test set\n",
        "test_confusion_mat = confusion_matrix(test_true_labels, test_predicted_sentiments)\n",
        "\n",
        "# Plot the confusion matrix for the test set\n",
        "test_display = ConfusionMatrixDisplay(confusion_matrix=test_confusion_mat, display_labels=labels)\n",
        "test_display.plot(cmap='Blues')\n",
        "plt.title(\"Test Set - Confusion Matrix\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rA9K_8z8nRL"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the data\n",
        "data_val = test_df[['Content', 'Sentiment']]\n",
        "data_val['Sentiment_label'] = pd.Categorical(data_val['Sentiment'])\n",
        "data_val['Sentiment'] = data_val['Sentiment_label'].cat.codes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Extract the training and testing texts and labels\n",
        "val_texts = data_val['Content'].tolist()\n",
        "val_labels = data_val['Sentiment'].tolist()\n",
        "\n",
        "# Tokenize the texts\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "num_labels = len(data_val['Sentiment_label'].cat.categories)\n",
        "val_labels_encoded = tf.one_hot(val_labels, num_labels)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels_encoded))\n",
        "\n",
        "# Make predictions on the training and test datasets\n",
        "\n",
        "val_predictions = model.predict(val_dataset.batch(64))\n",
        "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
        "\n",
        "# Convert the predicted labels to their original sentiment categories\n",
        "val_predicted_sentiments = data_val['Sentiment_label'].cat.categories[val_predicted_labels]\n",
        "\n",
        "# Convert the true labels to their original sentiment categories\n",
        "val_true_labels = data_val['Sentiment_label']\n",
        "\n",
        "# Calculate the classification report for the training set\n",
        "val_classification_rep = classification_report(val_true_labels, val_predicted_sentiments)\n",
        "print(\"Training Set - Classification Report:\\n\", val_classification_rep)\n",
        "\n",
        "# Generate the confusion matrix for the training set\n",
        "val_confusion_mat = confusion_matrix(val_true_labels, val_predicted_sentiments)\n",
        "\n",
        "# Get the unique labels/categories from the true labels\n",
        "labels = np.unique(val_true_labels)\n",
        "\n",
        "# Plot the confusion matrix for the training set\n",
        "val_display = ConfusionMatrixDisplay(confusion_matrix=val_confusion_mat, display_labels=labels)\n",
        "val_display.plot(cmap='Blues')\n",
        "plt.title(\"Validation Set - Confusion Matrix\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
